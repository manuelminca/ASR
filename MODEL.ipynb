{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils import data\n",
    "import os   \n",
    "#from generator import Dataset_CSV\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def listdir_nohidden(path):\n",
    "    for f in os.listdir(path):\n",
    "        if not f.startswith('.'):\n",
    "            yield f\n",
    "\n",
    "\n",
    "class Dataset_CSV_train(data.Dataset):\n",
    "    'Characterizes a dataset for PyTorch'\n",
    "    def __init__(self, location):\n",
    "        'Initialization'\n",
    "        self.locationX = location + '/data/'\n",
    "        self.locationY = location + '/labels/'\n",
    "        files = listdir_nohidden(self.locationX)\n",
    "        self.list_IDs = list(filter(lambda f: f.endswith('.csv'), files))\n",
    "        \n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        #return 96\n",
    "        return len(self.list_IDs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "    \n",
    "        ID = self.list_IDs[index]\n",
    "\n",
    "        # Load data and get label\n",
    "        \n",
    "        df_data = pd.read_csv(self.locationX + ID)\n",
    "        df_labels = pd.read_csv(self.locationY + ID)        \n",
    "        size = df_data.shape[0]\n",
    "        \n",
    "        #for cqt\n",
    "        minsize = 2048\n",
    "        #for mfcc\n",
    "        #minsize = 5520\n",
    "        \n",
    "        try:\n",
    "            position = random.randrange(0, size - minsize) #The start of the slice\n",
    "        except ValueError:\n",
    "            position = 0\n",
    "            \n",
    "        X = np.empty((minsize, 5, df_data.shape[1])) #5520 because its the smallest song size\n",
    "        Y = np.zeros((minsize,88))\n",
    "\n",
    "        auxiliary = 0\n",
    "\n",
    "        for j in range(position, position+minsize - 5):\n",
    "\n",
    "            if j == position:\n",
    "                for k in range(5):\n",
    "                    #Creating the 5 windows image\n",
    "                    X[auxiliary][k] = df_data.iloc[j + k]\n",
    "                if j < (position + minsize - 2):\n",
    "                    #Creating the labels, taking the label of the mid window\n",
    "                    Y[auxiliary] = df_labels.iloc[j + 2]\n",
    "\n",
    "                auxiliary+=1\n",
    "            else:\n",
    "                for k in range(5):\n",
    "                    if k < 3:\n",
    "                      #Copying the values of the previous image to avoid data repetition\n",
    "                      X[auxiliary][k] = X[auxiliary - 1][k + 1]\n",
    "                    else:\n",
    "                        #print(X.shape)\n",
    "                        #\n",
    "                        #print(k)\n",
    "                        #print(j)\n",
    "                        #\n",
    "                        #print(df_data.shape)\n",
    "                        X[auxiliary][k] = df_data.iloc[j + k]\n",
    "\n",
    "\n",
    "                if j < (position + minsize - 2):\n",
    "                    #Creating the labels, taking the label of the mid window\n",
    "                    Y[auxiliary] = df_labels.iloc[j + 2]\n",
    "\n",
    "                auxiliary+=1\n",
    "\n",
    "        #Normalise X using min-max normalisation\n",
    "        \n",
    "        #For filterbanks\n",
    "        #max_value = 120.1003100275508\n",
    "        #min_value = -330.7445963332333\n",
    "        \n",
    "        #For MFCC\n",
    "        #max_value = 841.837813253034\n",
    "        #min_value = -831.0670773337255\n",
    "        \n",
    "        #For CQT\n",
    "        max_value = 41.775738\n",
    "        min_value = 0\n",
    "        \n",
    "        x_norm = (X  - min_value) / (max_value - min_value)\n",
    "        x_norm = np.reshape(x_norm,(minsize,1, 5, df_data.shape[1]))\n",
    "        \n",
    "        \n",
    "        #for i,row in enumerate(Y):\n",
    "        #    yes = np.where(row==1)[0]\n",
    "        #    for element in yes:\n",
    "        #        if element -1 >= 0:\n",
    "        #            Y[i,element-1] = 1\n",
    "        #        if element +1 <= 87:\n",
    "        #            Y[i,element+1] = 1\n",
    "        \n",
    "        return x_norm, Y\n",
    "\n",
    "\n",
    "class Dataset_CSV_test(data.Dataset):\n",
    "    'Characterizes a dataset for PyTorch'\n",
    "    def __init__(self, location):\n",
    "        'Initialization'\n",
    "        self.locationX = location + '/data/'\n",
    "        self.locationY = location + '/labels/'\n",
    "        files = listdir_nohidden(self.locationX)\n",
    "        self.list_IDs = list(filter(lambda f: f.endswith('.csv'), files))\n",
    "        \n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.list_IDs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "    \n",
    "        ID = self.list_IDs[index]\n",
    "\n",
    "        # Load data and get label\n",
    "        \n",
    "        df_data = pd.read_csv(self.locationX + ID)\n",
    "        df_labels = pd.read_csv(self.locationY + ID)        \n",
    "        size = df_data.shape[0]\n",
    "        \n",
    "        #for mfcc\n",
    "        #position = 3500 #The shortest song has 9263 windows, so if all the songs start at the window 3500\n",
    "                         #We are avoiding the start of the song when there's nothing being played.\n",
    "        #minsize = 5520\n",
    "        \n",
    "        #for CQT\n",
    "        position = 172\n",
    "        minsize = 2048\n",
    "        \n",
    "        \n",
    "        X = np.empty((minsize, 5, df_data.shape[1])) #minsize because its the smallest song size\n",
    "        Y = np.zeros((minsize,88))\n",
    "\n",
    "        auxiliary = 0\n",
    "\n",
    "        for j in range(position, position+minsize -5):\n",
    "\n",
    "            if j == position:\n",
    "                for k in range(5):\n",
    "                    #Creating the 5 windows image\n",
    "                    X[auxiliary][k] = df_data.iloc[j + k]\n",
    "                if j < (position + minsize - 2):\n",
    "                    #Creating the labels, taking the label of the mid window\n",
    "                    Y[auxiliary] = df_labels.iloc[j + 2]\n",
    "\n",
    "                auxiliary+=1\n",
    "            else:\n",
    "                for k in range(5):\n",
    "                    if k < 3:\n",
    "                      #Copying the values of the previous image to avoid data repetition\n",
    "                      X[auxiliary][k] = X[auxiliary - 1][k + 1]\n",
    "                    else:\n",
    "                        X[auxiliary][k] = df_data.iloc[j + k]\n",
    "\n",
    "                if j < (position + minsize - 2):\n",
    "                    #Creating the labels, taking the label of the mid window\n",
    "                    Y[auxiliary] = df_labels.iloc[j + 2]\n",
    "\n",
    "                auxiliary+=1\n",
    "\n",
    "        #Normalise X using min-max normalisation\n",
    "        \n",
    "        #For filterbanks\n",
    "        #max_value = 120.1003100275508\n",
    "        #min_value = -330.7445963332333\n",
    "        \n",
    "        #For MFCC\n",
    "        #max_value = 841.837813253034\n",
    "        #min_value = -831.0670773337255\n",
    "        \n",
    "        #For CQT\n",
    "        max_value = 41.775738\n",
    "        min_value = 0\n",
    "        \n",
    "        x_norm = (X  - min_value) / (max_value - min_value)\n",
    "        x_norm = np.reshape(x_norm,(minsize,1, 5, df_data.shape[1]))\n",
    "        \n",
    "        #for i,row in enumerate(Y):\n",
    "        #    yes = np.where(row==1)[0]\n",
    "        #    for element in yes:\n",
    "        #        if element -1 >= 0:\n",
    "        #            Y[i,element-1] = 1\n",
    "        #        if element +1 <= 87:\n",
    "        #            Y[i,element+1] = 1\n",
    "        #\n",
    "        \n",
    "        return x_norm, Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainGenerator = Dataset_CSV_train(\"finalDatasetCQT/train/\")\n",
    "trainloader = DataLoader(trainGenerator, batch_size=16,\n",
    "                        shuffle=True, num_workers=4)\n",
    "\n",
    "testGenerator = Dataset_CSV_test(\"finalDatasetCQT/test/\")\n",
    "testloader = DataLoader(testGenerator, batch_size=2,\n",
    "                        shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = trainGenerator[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 1, 5, 384)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shortNet(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(1, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (pool1): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  (pool2): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=18048, out_features=88, bias=True)\n",
      "  (tanh): Tanh()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch import sigmoid\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 8, kernel_size=(1,3))\n",
    "        self.conv1_bn = nn.BatchNorm2d(8)\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=(3,3))\n",
    "        self.conv2_bn = nn.BatchNorm2d(16)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(16, 16, kernel_size=(1,3))\n",
    "        self.conv3_bn = nn.BatchNorm2d(16)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d((1, 2))\n",
    "        self.drop = torch.nn.Dropout(p=0.5)\n",
    "        self.fc1 = nn.Linear(3*46*16, 256)\n",
    "        self.fc2 = nn.Linear(256, 88)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.pool(F.relu(self.conv1_bn(self.conv1(x))))\n",
    "        #print(\"1 \", x.shape)\n",
    "        x = self.pool(F.relu(self.conv2_bn(self.conv2(x))))\n",
    "        x = self.drop(self.pool(F.relu(self.conv3_bn(self.conv3(x)))))\n",
    "        #print(\"2 \", x.shape)\n",
    "        x = x.view(-1, 3*46*16)\n",
    "        x = self.drop(F.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return sigmoid(x)\n",
    "\n",
    "    \n",
    "class shortNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(shortNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=(1,5), stride=1)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=(3,3))\n",
    "        \n",
    "        self.pool1 = nn.MaxPool2d((1, 2), stride = (1,2))\n",
    "        self.pool2 = nn.MaxPool2d((1, 2))\n",
    "        self.drop = torch.nn.Dropout(p=0.5)\n",
    "        self.fc1 = nn.Linear(64*3*94, 88)\n",
    "        self.tanh = torch.nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.tanh(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.tanh(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.pool2(x)\n",
    "        x = x.view(-1, 64*3*94)\n",
    "        x = self.fc1(x)\n",
    "        return sigmoid(x)    \n",
    "\n",
    "class longNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(longNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=(3,3), padding=1)\n",
    "        self.conv1_bn = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 16, kernel_size=(3,3))\n",
    "        self.conv2_bn = nn.BatchNorm2d(16)\n",
    "        self.dropbig = torch.nn.Dropout(p=0.5)\n",
    "        self.dropsmall = torch.nn.Dropout(p=0.25)\n",
    "        self.conv3 = nn.Conv2d(16, 32, kernel_size=(1,3))\n",
    "        self.conv3_bn = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d((1, 2))\n",
    "       \n",
    "        self.linear1 = nn.Linear(32*3*46, 64)\n",
    "        self.linear1_bn = nn.BatchNorm1d(64)\n",
    "        self.linear2 = nn.Linear(64, 128)\n",
    "        self.linear2_bn = nn.BatchNorm1d(128)\n",
    "        self.linear3 = nn.Linear(128, 88)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(\"0 \", x.shape)\n",
    "        \n",
    "        #print(\"XX0: \", x)\n",
    "        x = self.conv1(x)\n",
    "        #print(\"XX1: \", x)\n",
    "        x = F.relu(x)\n",
    "        #print(\"XX2: \", x)\n",
    "        #x = self.conv1_bn(x)\n",
    "        #print(\"XX3: \", x)\n",
    "        x = self.pool(x)\n",
    "        #print(\"XX4: \", x)\n",
    "        x = self.dropsmall(x)\n",
    "        #print(\"XX5: \", x)\n",
    "        #print(\"1: \", x)\n",
    "        #print(\"1 \", x.shape)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        #x = self.conv2_bn(x)\n",
    "        x = self.dropsmall(self.pool(x))\n",
    "        #print(\"2: \", x)\n",
    "        #print(\"2 \", x.shape)\n",
    "        x = self.dropsmall(self.pool(F.relu(self.conv3_bn(self.conv3(x)))))\n",
    "        #print(\"3: \", x)\n",
    "        #print(\"3 \", x.shape)\n",
    "        x = x.view(-1, np.prod(x.shape[1:]))\n",
    "        x = self.dropsmall(F.relu(self.linear1_bn(self.linear1(x))))\n",
    "        #print(\"4: \", x)\n",
    "        x = self.dropbig(F.relu(self.linear2_bn(self.linear2(x))))\n",
    "        #print(\"5: \", x)\n",
    "        x = self.linear3(x)\n",
    "        #print(\"6: \", x)\n",
    "        return sigmoid(x)\n",
    "\n",
    "\n",
    "\n",
    "net = shortNet()\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "device = 'cpu'\n",
    "net = net.to(device)\n",
    "\n",
    "print(repr(net))\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n"
     ]
    }
   ],
   "source": [
    "#ShortNet\n",
    "loss_values = []\n",
    "val_accs = []\n",
    "\n",
    "for epoch in range(3):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    aux = 0\n",
    "    net.train() # pytorch way to make model trainable.\n",
    "    print(\"Training\")\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        for j in range(inputs.shape[0]):\n",
    "            X = inputs[j].float().to(device)\n",
    "            Y = labels[j].float().to(device)\n",
    "        \n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(X)\n",
    "            #print(outputs)\n",
    "            loss = criterion(outputs, Y)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss_values.append(loss.item())\n",
    "\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if aux % 20 == 19:    # print every 10 mini-batches\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                    (epoch + 1, i + 1, running_loss / 20))\n",
    "                loss_values.append(running_loss / 20)\n",
    "                running_loss = 0.0\n",
    "            aux += 1  \n",
    "\n",
    "\n",
    "    \n",
    "    test_loss = []\n",
    "    test_accuracy = []\n",
    "    predicted_correctly = 0\n",
    "    total = 0\n",
    "    aux = 0\n",
    "    print(\"Testing\")\n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        for i, data in enumerate(testloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "        \n",
    "            for j in range(inputs.shape[0]):\n",
    "                X = inputs[j].float().to(device)\n",
    "                Y = labels[j].float().to(device)\n",
    "\n",
    "                output = net(X)\n",
    "                loss = criterion(output, Y)\n",
    "                test_loss.append(loss.item())\n",
    "                #print(\"Validation loss: \", loss.item())\n",
    "\n",
    "                ##_, max_indices = torch.max(output.data,1, keepdim=True)\n",
    "                #max_indices = np.where(output.cpu() > 0.55, 1, 0)\n",
    "                #n = np.prod(max_indices.shape) #max_indices.size(0) #index 0 for extracting the # of elements\n",
    "                #train_acc = (torch.tensor(max_indices) == Y).sum()/n\n",
    "                #test_accuracy.append(train_acc.item())\n",
    "                \n",
    "                max_indices = np.where(output.cpu() > 0.55, 1, 0)\n",
    "\n",
    "                for i, row in enumerate(Y):\n",
    "                    for j, col in enumerate(row):\n",
    "                        \n",
    "                        if max_indices[i][j].item() == 1:\n",
    "                            #Predicted ones\n",
    "                            aux += 1\n",
    "                        if col.item() == 1:\n",
    "                            #Label with a one\n",
    "                            total+= 1\n",
    "                        if col.item() == 0 and max_indices[i][j].item() == 0:\n",
    "                            continue\n",
    "                        elif col.item() == 1 and max_indices[i][j].item() == 1:\n",
    "                            #print(\"PREDICTED CORRECTLY\")\n",
    "                            predicted_correctly += 1\n",
    "                        else:\n",
    "                            continue\n",
    "                \n",
    "                \n",
    "                test_acc = predicted_correctly/total\n",
    "                test_accuracy.append(test_acc)\n",
    "    \n",
    "    print(\"##########\")\n",
    "    print(\"EPOCH \", epoch)\n",
    "    print(\"##########\")\n",
    "    \n",
    "    print(\"##TRAINING STATS##\\n\")\n",
    "    print(\"Loss: \", sum(loss_values)/ len(loss_values))\n",
    "    #print(\"Acc: \", sum(test_accuracy)/ len(test_accuracy))\n",
    "    print(\"######\\n\")\n",
    "    \n",
    "    print(\"##VALIDATION STATS##\\n\")\n",
    "    print(\"Loss: \", sum(test_loss)/ len(test_loss))\n",
    "    acc = sum(test_accuracy)/ len(test_accuracy)\n",
    "    print(\"Acc: \", acc)\n",
    "    val_accs.append(acc)\n",
    "    print(\"Predicted correctly: \", predicted_correctly, \"out of: \", total)\n",
    "    print(\"Numbers of ones predicted in total \", aux)\n",
    "    print(\"######\\n\")\n",
    "    torch.save(net, \"LastModel.pt\")\n",
    "    \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "[1,     2] loss: 0.142\n",
      "[1,     3] loss: 0.137\n",
      "[1,     4] loss: 0.121\n",
      "[1,     5] loss: 0.162\n",
      "[1,     7] loss: 0.152\n",
      "[1,     8] loss: 0.116\n",
      "[1,     9] loss: 0.140\n",
      "[1,    10] loss: 0.147\n",
      "[1,    12] loss: 0.140\n",
      "[1,    13] loss: 0.142\n",
      "[1,    14] loss: 0.152\n",
      "[1,    15] loss: 0.141\n",
      "[1,    17] loss: 0.138\n",
      "[1,    18] loss: 0.133\n",
      "[1,    19] loss: 0.138\n",
      "[1,    20] loss: 0.118\n",
      "Testing\n",
      "##########\n",
      "EPOCH  0\n",
      "##########\n",
      "##TRAINING STATS##\n",
      "\n",
      "Loss:  0.13869537137798033\n",
      "######\n",
      "\n",
      "##VALIDATION STATS##\n",
      "\n",
      "Loss:  0.8395478129386902\n",
      "Acc:  0.07310341515656367\n",
      "Predicted correctly:  456 out of:  10810\n",
      "Numbers of ones predicted in total  35840\n",
      "######\n",
      "\n",
      "Training\n",
      "[2,     2] loss: 0.151\n",
      "[2,     3] loss: 0.134\n",
      "[2,     4] loss: 0.139\n",
      "[2,     5] loss: 0.144\n",
      "[2,     7] loss: 0.126\n",
      "[2,     8] loss: 0.136\n",
      "[2,     9] loss: 0.136\n",
      "[2,    10] loss: 0.147\n",
      "[2,    12] loss: 0.123\n",
      "[2,    13] loss: 0.152\n",
      "[2,    14] loss: 0.138\n",
      "[2,    15] loss: 0.133\n",
      "[2,    17] loss: 0.138\n",
      "[2,    18] loss: 0.124\n",
      "[2,    19] loss: 0.134\n",
      "[2,    20] loss: 0.129\n",
      "Testing\n",
      "##########\n",
      "EPOCH  1\n",
      "##########\n",
      "##TRAINING STATS##\n",
      "\n",
      "Loss:  0.1376395564089762\n",
      "######\n",
      "\n",
      "##VALIDATION STATS##\n",
      "\n",
      "Loss:  0.8065969169139862\n",
      "Acc:  0.05114960527318334\n",
      "Predicted correctly:  615 out of:  10810\n",
      "Numbers of ones predicted in total  46080\n",
      "######\n",
      "\n",
      "Training\n",
      "[3,     2] loss: 0.147\n",
      "[3,     3] loss: 0.142\n",
      "[3,     4] loss: 0.174\n",
      "[3,     5] loss: 0.153\n",
      "[3,     7] loss: 0.127\n",
      "[3,     8] loss: 0.135\n",
      "[3,     9] loss: 0.134\n",
      "[3,    10] loss: 0.140\n",
      "[3,    12] loss: 0.127\n",
      "[3,    13] loss: 0.129\n",
      "[3,    14] loss: 0.147\n",
      "[3,    15] loss: 0.146\n",
      "[3,    17] loss: 0.139\n",
      "[3,    18] loss: 0.133\n",
      "[3,    19] loss: 0.151\n",
      "[3,    20] loss: 0.115\n",
      "Testing\n",
      "##########\n",
      "EPOCH  2\n",
      "##########\n",
      "##TRAINING STATS##\n",
      "\n",
      "Loss:  0.1384111434783942\n",
      "######\n",
      "\n",
      "##VALIDATION STATS##\n",
      "\n",
      "Loss:  0.9087282717227936\n",
      "Acc:  0.1284032588828477\n",
      "Predicted correctly:  1497 out of:  10810\n",
      "Numbers of ones predicted in total  40960\n",
      "######\n",
      "\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "#ShortNet\n",
    "loss_values = []\n",
    "val_accs = []\n",
    "\n",
    "for epoch in range(3):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    aux = 0\n",
    "    net.train() # pytorch way to make model trainable.\n",
    "    print(\"Training\")\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        for j in range(inputs.shape[0]):\n",
    "            X = inputs[j].float().to(device)\n",
    "            Y = labels[j].float().to(device)\n",
    "        \n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(X)\n",
    "            #print(outputs)\n",
    "            loss = criterion(outputs, Y)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss_values.append(loss.item())\n",
    "\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if aux % 20 == 19:    # print every 10 mini-batches\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                    (epoch + 1, i + 1, running_loss / 20))\n",
    "                loss_values.append(running_loss / 20)\n",
    "                running_loss = 0.0\n",
    "            aux += 1  \n",
    "\n",
    "\n",
    "    \n",
    "    test_loss = []\n",
    "    test_accuracy = []\n",
    "    predicted_correctly = 0\n",
    "    total = 0\n",
    "    aux = 0\n",
    "    print(\"Testing\")\n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        for i, data in enumerate(testloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "        \n",
    "            for j in range(inputs.shape[0]):\n",
    "                X = inputs[j].float().to(device)\n",
    "                Y = labels[j].float().to(device)\n",
    "\n",
    "                output = net(X)\n",
    "                loss = criterion(output, Y)\n",
    "                test_loss.append(loss.item())\n",
    "                #print(\"Validation loss: \", loss.item())\n",
    "\n",
    "                ##_, max_indices = torch.max(output.data,1, keepdim=True)\n",
    "                #max_indices = np.where(output.cpu() > 0.55, 1, 0)\n",
    "                #n = np.prod(max_indices.shape) #max_indices.size(0) #index 0 for extracting the # of elements\n",
    "                #train_acc = (torch.tensor(max_indices) == Y).sum()/n\n",
    "                #test_accuracy.append(train_acc.item())\n",
    "                \n",
    "                max_indices = np.where(output.cpu() > 0.55, 1, 0)\n",
    "\n",
    "                for i, row in enumerate(Y):\n",
    "                    for j, col in enumerate(row):\n",
    "                        \n",
    "                        if max_indices[i][j].item() == 1:\n",
    "                            #Predicted ones\n",
    "                            aux += 1\n",
    "                        if col.item() == 1:\n",
    "                            #Label with a one\n",
    "                            total+= 1\n",
    "                        if col.item() == 0 and max_indices[i][j].item() == 0:\n",
    "                            continue\n",
    "                        elif col.item() == 1 and max_indices[i][j].item() == 1:\n",
    "                            #print(\"PREDICTED CORRECTLY\")\n",
    "                            predicted_correctly += 1\n",
    "                        else:\n",
    "                            continue\n",
    "                \n",
    "                \n",
    "                test_acc = predicted_correctly/total\n",
    "                test_accuracy.append(test_acc)\n",
    "    \n",
    "    print(\"##########\")\n",
    "    print(\"EPOCH \", epoch)\n",
    "    print(\"##########\")\n",
    "    \n",
    "    print(\"##TRAINING STATS##\\n\")\n",
    "    print(\"Loss: \", sum(loss_values)/ len(loss_values))\n",
    "    #print(\"Acc: \", sum(test_accuracy)/ len(test_accuracy))\n",
    "    print(\"######\\n\")\n",
    "    \n",
    "    print(\"##VALIDATION STATS##\\n\")\n",
    "    print(\"Loss: \", sum(test_loss)/ len(test_loss))\n",
    "    acc = sum(test_accuracy)/ len(test_accuracy)\n",
    "    print(\"Acc: \", acc)\n",
    "    val_accs.append(acc)\n",
    "    print(\"Predicted correctly: \", predicted_correctly, \"out of: \", total)\n",
    "    print(\"Numbers of ones predicted in total \", aux)\n",
    "    print(\"######\\n\")\n",
    "    torch.save(net, \"LastModel.pt\")\n",
    "    \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_values = []\n",
    "\n",
    "for epoch in range(5):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    aux = 0\n",
    "    #net.train() # pytorch way to make model trainable.\n",
    "    print(\"Training\")\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        for j in range(inputs.shape[0]):\n",
    "            X = inputs[j].float()\n",
    "            Y = labels[j].float()\n",
    "        \n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(X)\n",
    "            \n",
    "            loss = criterion(outputs, Y)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_values.append(loss.item())\n",
    "\n",
    "            #print(\"The loss is: \", loss.item())\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if aux % 20 == 19:    # print every 10 mini-batches\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                    (epoch + 1, i + 1, running_loss / 20))\n",
    "                loss_values.append(running_loss / 20)\n",
    "                running_loss = 0.0\n",
    "            aux += 1  \n",
    "\n",
    "\n",
    "    \n",
    "    test_loss = []\n",
    "    test_accuracy = []\n",
    "    print(\"Testing\")\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(testloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            for j in range(inputs.shape[0]):\n",
    "                X = inputs[j].float()\n",
    "                Y = labels[j].float()\n",
    "\n",
    "                output = net(X)\n",
    "                loss = criterion(output, Y)\n",
    "                test_loss.append(loss.item())\n",
    "                #print(\"Validation loss: \", loss.item())\n",
    "\n",
    "                ##_, max_indices = torch.max(output.data,1, keepdim=True)\n",
    "                #max_indices = np.where(output.cpu() > 0.55, 1, 0)\n",
    "                #n = np.prod(max_indices.shape) #max_indices.size(0) #index 0 for extracting the # of elements\n",
    "                #train_acc = (torch.tensor(max_indices) == Y).sum()/n\n",
    "                #test_accuracy.append(train_acc.item())\n",
    "                \n",
    "                max_indices = np.where(output.cpu() > 0.5, 1, 0)\n",
    "                predicted_correctly = 0\n",
    "                total = 0\n",
    "                for i, row in enumerate(Y):\n",
    "                    for j, col in enumerate(row):\n",
    "                        if col.item() == 1:\n",
    "                            total+= 1\n",
    "                        if col.item() == 0 and max_indices[i][j].item() == 0:\n",
    "                            continue\n",
    "                        elif col.item() == 1 and max_indices[i][j].item() == 1:\n",
    "                            #print(\"PREDICTED CORRECTLY\")\n",
    "                            predicted_correctly += 1\n",
    "                        else:\n",
    "                            continue\n",
    "                train_acc = predicted_correctly/total\n",
    "                test_accuracy.append(train_acc)\n",
    "    \n",
    "    print(\"##########\")\n",
    "    print(\"EPOCH \", epoch)\n",
    "    print(\"##########\")\n",
    "    \n",
    "    print(\"##TRAINING STATS##\\n\")\n",
    "    print(\"Loss: \", sum(loss_values)/ len(loss_values))\n",
    "    #print(\"Acc: \", sum(test_accuracy)/ len(test_accuracy))\n",
    "    print(\"######\\n\")\n",
    "    \n",
    "    print(\"##VALIDATION STATS##\\n\")\n",
    "    print(\"Loss: \", sum(test_loss)/ len(test_loss))\n",
    "    print(\"Acc: \", sum(test_accuracy)/ len(test_accuracy))\n",
    "    print(\"Predicted correctly: \", predicted_correctly, \"out of: \", total)\n",
    "    print(\"######\\n\")\n",
    "    \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net, \"LastModel47percent.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_, max_indices = torch.max(output.data,1, keepdim=True)\n",
    "max_indices = np.where(output.cpu() > 0.55, 1, 0)\n",
    "n = np.prod(max_indices.shape) #max_indices.size(0) #index 0 for extracting the # of elements\n",
    "train_acc = (torch.tensor(max_indices) == Y and not (torch.tensor(max_indices) == 0 and Y == 0)).sum()/n\n",
    "train_acc.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor((1,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_indices = np.where(output.cpu() > 0.5, 1, 0)\n",
    "total = 0\n",
    "for i, row in enumerate(Y):\n",
    "    for j, col in enumerate(row):\n",
    "        if col.item() == 0 and max_indices[i][j].item() == 0:\n",
    "            continue\n",
    "        elif col.item() == 1 and max_indices[i][j].item() == 1:\n",
    "            total += 1\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not(torch.tensor(max_indices) == 0 and Y == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"finalDataset/train/data/1727.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KERAS PART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D,Conv1D, MaxPooling2D, Activation, LSTM, Reshape, Convolution2D, BatchNormalization, Dense, Dropout, Flatten\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "model.add(Conv2D(8, kernel_size=(1,3),\n",
    "                 activation='relu',\n",
    "                 input_shape=(5,384,1)))\n",
    "model.add(MaxPooling2D(pool_size=(1, 2)))\n",
    "#model.add(BatchNormalization())\n",
    "#model.add(Activation('relu'))\n",
    "#model.add(Dropout(0.25))\n",
    "model.add(Conv2D(16, (3, 3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(1, 2)))\n",
    "\n",
    "model.add(Conv2D(16, (1, 3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(1, 2)))\n",
    "\n",
    "\n",
    "#model.add(Dropout(0.25))\n",
    "#model.add(Conv2D(16, (3, 3)))\n",
    "#model.add(BatchNormalization())\n",
    "#model.add(Activation('relu'))\n",
    "#model.add(MaxPooling2D(pool_size=(1, 2)))\n",
    "#model.add(Dropout(0.25))\n",
    "#model.add(Conv2D(32, (1, 3)))\n",
    "#model.add(BatchNormalization())\n",
    "#model.add(Activation('relu'))\n",
    "#model.add(MaxPooling2D(pool_size=(1, 2)))\n",
    "#model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "#model.add(Dense(64))\n",
    "#model.add(Dropout(0.25))\n",
    "#model.add(BatchNormalization())\n",
    "#model.add(Activation('relu'))\n",
    "#model.add(Dropout(0.5))\n",
    "model.add(Dense(88, activation='sigmoid'))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_12 (Conv2D)           (None, 5, 380, 32)        192       \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 5, 380, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 5, 190, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 3, 188, 64)        18496     \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 3, 188, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 3, 94, 64)         0         \n",
      "=================================================================\n",
      "Total params: 18,688\n",
      "Trainable params: 18,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(1,5), strides=(1,1),\n",
    "    activation='tanh',\n",
    "    input_shape=(5,384,1)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(MaxPooling2D(pool_size=(1,2), strides=(1,2)))\n",
    "model.add(Conv2D(64, (3,3), activation='tanh'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(MaxPooling2D(pool_size=(1,2)))\n",
    "\n",
    "#model.add(Flatten())\n",
    "##model.add(Dense(64, activation='sigmoid'))\n",
    "#model.add(Dense(num_classes, activation='sigmoid'))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: '5ads'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-225a9f27ad92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'5ads'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: '5ads'"
     ]
    }
   ],
   "source": [
    "int('5ads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
